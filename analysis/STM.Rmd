---
title: "NLP Project - STM"
output: html_notebook
---
```{r}
if (!require(devtools)) install.packages("devtools")
library(devtools)
devtools::install_github("mikajoh/tidystm", dependencies = TRUE)
devtools::install_github("milesdwilliams15/Better-Graphics-for-the-stm-Package-in-R", dependencies = TRUE)
```

```{r}
library(sf)
library(tidyverse)
library(stm)
library(topicmodels)
library(lda) 
library(quanteda)
library(quanteda.textplots)
library(slam)
library(quanteda)
library(dplyr)
library(ggplot2)
library(tidystm)
library(RColorBrewer)
library(ggthemes)
library(gridExtra)


# Define the color palette
my_palette <- brewer.pal(9, "Set1")  

# Set directory
DIR <- '.'
```

# Load data
```{r}
# Load data
lyrics_data <- read.csv(file.path(DIR, '../../data/top songs processed.csv'))
```

```{r}
# Remove all 2 letter words 
lyrics_data$lyrics <- gsub("\\b\\w{2}\\b", "", lyrics_data$lyrics)

# Add ID column 
lyrics_data <- lyrics_data %>%
  mutate(row_num = row_number())

# Build corpus
lyricsCorpus <- corpus(lyrics_data, docid_field = 'row_num', text_field = 'lyrics')

# Tokenize corpus
lyricsCorpus_tokenized <- tokens(lyricsCorpus)

#Convert our tokenized corpus into a documentÃ—term frequency
dfm_lyricsCorpus<- dfm(lyricsCorpus_tokenized)
```


# Plot genre per year

```{r}
# Step 1: Calculate total tags per year
yearly_totals <- lyrics_data %>%
  group_by(release.year) %>%
  summarise(total_tags = n(), .groups = 'drop')

# Step 2: Calculate tag counts per year
tag_counts_per_year <- lyrics_data %>%
  group_by(release.year, tag) %>%
  summarise(tag_count = n(), .groups = 'drop')

# Step 3: Join the totals with the counts and calculate percentages
tag_percentages <- tag_counts_per_year %>%
  left_join(yearly_totals, by = "release.year") %>%
  mutate(percentage = (tag_count / total_tags) * 100)

# Step 4: Plot the data
ggplot(tag_percentages, aes(x = release.year, y = percentage, group = tag, color = tag)) +
  geom_line() +
  theme_calc() +
  theme(panel.border = element_blank()) +
  labs(x = "Release Year", y = "Percentage (%)") +
  scale_color_discrete(name = "Genre", labels = c("Country", "Misc", "Pop", "Rap", "R & B", "Rock"))
ggsave("../figures/Genres over time.png", width = 23, height = 15, units = "cm")
```

# Filter corpus

## Attempt 1

```{r}
# Create custom stop words list
custom_list_stopwords <- c(stopwords("en"))

# Filter corpus
dfm_lyricsCorpus_filtered <- tokens(lyricsCorpus, remove_punct = TRUE, 
                                          remove_symbols = TRUE, remove_numbers = TRUE) %>% 
                            tokens_remove(custom_list_stopwords) %>%
                            tokens_wordstem() %>% 
                            tokens_ngrams(n = c(1, 2)) %>% 
                            dfm() %>% 
                            dfm_tolower() %>% 
                            dfm_trim(min_termfreq = 5, min_docfreq = 0.01, max_docfreq = 0.99, docfreq_type = "prop")
```


```{r, warning=FALSE}
textplot_wordcloud(dfm_lyricsCorpus_filtered, random_order = FALSE, rotation = 0.25, 
    color = RColorBrewer::brewer.pal(8, "Dark2"))
```

```{r}
# Calculate the document frequency for each term
# This computes how many documents each term appears in
doc_freqs <- docfreq(dfm_lyricsCorpus_filtered)

# Convert to a data frame, sort it, and create a column for term ID or rank
df_doc_freqs <- data.frame(term = names(doc_freqs), doc_freq = doc_freqs) %>%
  arrange(desc(doc_freq)) %>%
  mutate(term_id = row_number())

# Create the plot for document frequencies
ggplot(df_doc_freqs, aes(x = term_id, y = doc_freq)) +
  geom_bar(stat = "identity") +
  labs(title = "Document Frequency Distribution", x = "Term ID", y = "Document Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) # Hide x-axis text for clarity

```
## Attempt 2
```{r}
# Filter corpus
dfm_lyricsCorpus_filtered <- tokens(lyricsCorpus, remove_punct = TRUE, 
                                          remove_symbols = TRUE, remove_numbers = TRUE) %>% 
                            tokens_remove(custom_list_stopwords) %>%
                            tokens_wordstem() %>% 
                            tokens_ngrams(n = c(1, 2)) %>% 
                            dfm() %>% 
                            dfm_tolower() %>% 
                            dfm_trim(min_termfreq = 5, min_docfreq = 480, max_docfreq = 30000)
```


```{r, warning=FALSE}
textplot_wordcloud(dfm_lyricsCorpus_filtered, random_order = FALSE, rotation = 0.25, 
    color = RColorBrewer::brewer.pal(8, "Dark2"))
```


```{r}
# Calculate the document frequency for each term
# This computes how many documents each term appears in
doc_freqs <- docfreq(dfm_lyricsCorpus_filtered)

# Convert to a data frame, sort it, and create a column for term ID or rank
df_doc_freqs <- data.frame(term = names(doc_freqs), doc_freq = doc_freqs) %>%
  arrange(desc(doc_freq)) %>%
  mutate(term_id = row_number())

# Create the plot for document frequencies
ggplot(df_doc_freqs, aes(x = term_id, y = doc_freq)) +
  geom_bar(stat = "identity") +
  labs(title = "Document Frequency Distribution", x = "Term ID", y = "Document Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) # Hide x-axis text for clarity
```

# Run STM

```{r}
# Assuming dfm_lyricsCorpus_filtered is your document-term matrix
non_empty_docs <- which(rowSums(dfm_lyricsCorpus_filtered) > 0)

# Filter your DFM to keep only non-empty documents
dfm_lyricsCorpus_filtered <- dfm_lyricsCorpus_filtered[non_empty_docs, ]

# Prepare the metadata for STM
meta_data <- data.frame(date = lyrics_data$release.year, genre = lyrics_data$tag)
meta_data <- meta_data[non_empty_docs, ]
```

```{r}
# Run the STM model
stm_model <- stm(documents = dfm_lyricsCorpus_filtered, 
                 data = meta_data, 
                 K = 20, 
                 prevalence = ~ s(date) + genre + s(date)*genre, 
                 seed = 123, init.type = "Spectral")
```


```{r}
topic_labels <- c("Vocal Expressions: ", "Love: ", "Feelings and Friendship: ", "Conversational and Family: ", "Journey: ", "Dark Imagery: ", "Aspiration: ", "Party: ", "Male terms: ", "Change and Departure: ", "Desire: ", "Expletives: ", "Reflection: ", "Spirituality: ", "Sociopolitical: ", "Romance: ", "Street Life and Expletives: ", "Tonight: ", "Exclamations: ", "Knowledge: ")

plot(stm_model, type="summary", topic.names = topic_labels, text.cex = 0.9, main = "")

par(bty="n",lwd=2,xaxt="n") 
plot.STM(stm_model,type="summary",topic.names=topic_labels)
```

```{r}
labelTopics(stm_model, n = 7)
```
1. **Vocal Expressions**: 

2. **Love**: 

3. **Feelings and Friendship**: 

4. **Conversational and Family**: 

5. **Journey**: 

6. **Dark Imagery**: 

7. **Aspiration**: 

8. **Party**:

9. **Male Terms**: 

10. **Change and Departure**: 

11. **Desire**: 

12. **Expletives**: 

13. **Reflection**: 

14. **Spirituality**: 

15. **Sociopolitical**: 

16. **Romance**: 

17. **Street Life and Expletives**: 

18. **Tonight**: 

19. **Exclamations**: 

20. **Knowledge**: 

# Topics over time

```{r}
# Estimate time effect
time_effect <- estimateEffect(1:20 ~ s(date), stm_model, meta = meta_data, uncertainty = "Global")
effect <- extract.estimateEffect(time_effect, "date", model = stm_model, method = "pointestimate")
```


```{r}
# Filter for relevant topics
topics_data <- effect %>% 
  filter(topic %in% c(1:2))

# Now plot using ggplot2
ggplot(topics_data, aes(x = covariate.value, y = estimate, group = topic, color = as.factor(topic))) +
  geom_line() +
  scale_color_manual(values = my_palette, labels = c("Topic 1", "Topic 2")) +
  theme_calc() +
  labs(x = "Release Year", y = "Expected Topic Proportions", color = "Topic")

```



## Optimise for K

```{r}
# Optimise for K
system.time(
  searchK <- searchK(documents = dfm_lyricsCorpus_filtered, 
                     K = c(10,15, 20, 25, 30,40), #specify K to try
                      N = 4800, # Use 10% of observations to determine optimum number of topics
                     proportion = 0.5, # hold out 50% to use for testing
                     heldout.seed = 1234, 
                     M = 10, # number of random initializations to perform
                     cores = 1, # default
                     prevalence =~ s(date) + genre + s(date)*genre,
                     max.em.its = 75, # Max iterations
                     data = meta_data,
                     init.type = "Spectral",
                     verbose=TRUE)
)
```

```{r}
# Extracting the data from the searchK list
K_values <- searchK$results$K  
heldout_likelihood <- searchK$results$heldout
semcoh <- searchK$results$semcoh
residual<- searchK$results$residual
lowerbound <- searchK$results$lbound

# Creating a data frame for plotting
plot_data <- data.frame(
  K = unlist(K_values),
  HeldOutLikelihood = unlist(heldout_likelihood),
  semcoh = unlist(semcoh),
  residual = unlist(residual),
  lowerbound = unlist(lowerbound)
)

# Held-Out Likelihood Plot
plot_heldout <- ggplot(plot_data, aes(x = K, y = HeldOutLikelihood)) +
  geom_line(color = my_palette[1]) +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 90)) +
  labs(x = "",
       y = "Held-Out Likelihood")

# Semantic Coherence Plot
plot_semcoh <- ggplot(plot_data, aes(x = K, y = semcoh)) +
  geom_line(color = my_palette[2]) +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 90)) +
  labs(x = "",
       y = "Semantic Coherence")

# Residual Plot
plot_residual <- ggplot(plot_data, aes(x = K, y = residual)) +
  geom_line(color = my_palette[3]) +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 90)) +
  labs(x = "Number of Topics (K)",
       y = "Residual")

# Lower Bound Plot
plot_lowerbound <- ggplot(plot_data, aes(x = K, y = lowerbound)) +
  geom_line(color = my_palette[4]) +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 90)) +
  labs(x = "Number of Topics (K)",
       y = "Lower Bound")

# Combining the graphs into one plot
combined_plot <- grid.arrange(plot_heldout, plot_semcoh, plot_residual, plot_lowerbound, ncol = 2)

# Combining the graphs into one plot
combined_plot <- grid.arrange(plot_heldout, plot_semcoh, plot_residual, plot_lowerbound, ncol = 2)
ggsave("../figures/Model diagnostics.png", width = 23, height = 15, units = "cm")
```

