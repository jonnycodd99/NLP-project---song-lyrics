---
title: "NLP Project - STM"
output: html_notebook
---
```{r}
if (!require(devtools)) install.packages("devtools")
library(devtools)
devtools::install_github("mikajoh/tidystm", dependencies = TRUE)
devtools::install_github("milesdwilliams15/Better-Graphics-for-the-stm-Package-in-R", dependencies = TRUE)
```

```{r}
library(sf)
library(tidyverse)
library(stm)
library(topicmodels)
library(lda) 
library(quanteda)
library(quanteda.textplots)
library(slam)
library(quanteda)
library(dplyr)
library(ggplot2)
library(tidystm)
library(RColorBrewer)
library(ggthemes)
library(gridExtra)
library(tidytext)

# Define the color palette
my_palette <- brewer.pal(9, "Set1")  

# Set directory
DIR <- '.'
```

# Load data

```{r}
# Load data
lyrics_data <- read.csv(file.path(DIR, '../../data/top songs processed.csv'))
```

```{r}
# Remove all 2 letter words 
lyrics_data$lyrics <- gsub("\\b\\w{2}\\b", "", lyrics_data$lyrics)

# Add ID column 
lyrics_data <- lyrics_data %>%
  mutate(row_num = row_number())

# Build corpus
lyricsCorpus <- corpus(lyrics_data, docid_field = 'row_num', text_field = 'lyrics')

# Tokenize corpus
lyricsCorpus_tokenized <- tokens(lyricsCorpus)

#Convert our tokenized corpus into a documentÃ—term frequency
dfm_lyricsCorpus<- dfm(lyricsCorpus_tokenized)
```


# Plot genre per year

```{r}
# Step 1: Calculate total tags per year
yearly_totals <- lyrics_data %>%
  group_by(release.year) %>%
  summarise(total_tags = n(), .groups = 'drop')

# Step 2: Calculate tag counts per year
tag_counts_per_year <- lyrics_data %>%
  group_by(release.year, tag) %>%
  summarise(tag_count = n(), .groups = 'drop')

# Step 3: Join the totals with the counts and calculate percentages
tag_percentages <- tag_counts_per_year %>%
  left_join(yearly_totals, by = "release.year") %>%
  mutate(percentage = (tag_count / total_tags) * 100)

# Step 4: Plot the data
ggplot(tag_percentages, aes(x = release.year, y = percentage, group = tag, color = tag)) +
  geom_line() +
  theme_calc() +
  theme(panel.border = element_blank()) +
  labs(x = "Release Year", y = "Percentage (%)") +
  scale_color_discrete(name = "Genre", labels = c("Country", "Misc", "Pop", "Rap", "R & B", "Rock"))
ggsave("../figures/Genres over time.png", width = 23, height = 15, units = "cm")
```

# Filter corpus

## Attempt 1

```{r}
# Create custom stop words list
custom_list_stopwords <- c(stopwords("en"))

# Filter corpus
dfm_lyricsCorpus_filtered <- tokens(lyricsCorpus, remove_punct = TRUE, 
                                          remove_symbols = TRUE, remove_numbers = TRUE) %>% 
                            tokens_remove(custom_list_stopwords) %>%
                            tokens_wordstem() %>% 
                            tokens_ngrams(n = c(1, 2)) %>% 
                            dfm() %>% 
                            dfm_tolower() %>% 
                            dfm_trim(min_termfreq = 5, min_docfreq = 0.01, max_docfreq = 0.99, docfreq_type = "prop")
```


```{r, warning=FALSE}
textplot_wordcloud(dfm_lyricsCorpus_filtered, random_order = FALSE, rotation = 0.25, 
    color = RColorBrewer::brewer.pal(8, "Dark2"))
```

```{r}
# Calculate the document frequency for each term
# This computes how many documents each term appears in
doc_freqs <- docfreq(dfm_lyricsCorpus_filtered)

# Convert to a data frame, sort it, and create a column for term ID or rank
df_doc_freqs <- data.frame(term = names(doc_freqs), doc_freq = doc_freqs) %>%
  arrange(desc(doc_freq)) %>%
  mutate(term_id = row_number())

# Create the plot for document frequencies
ggplot(df_doc_freqs, aes(x = term_id, y = doc_freq)) +
  geom_bar(stat = "identity") +
  labs(title = "Document Frequency Distribution", x = "Term ID", y = "Document Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) # Hide x-axis text for clarity

```
## Attempt 2
```{r}
# Filter corpus
dfm_lyricsCorpus_filtered <- tokens(lyricsCorpus, remove_punct = TRUE, 
                                          remove_symbols = TRUE, remove_numbers = TRUE) %>% 
                            tokens_remove(custom_list_stopwords) %>%
                            tokens_wordstem() %>% 
                            tokens_ngrams(n = c(1, 2)) %>% 
                            dfm() %>% 
                            dfm_tolower() %>% 
                            dfm_trim(min_termfreq = 5, min_docfreq = 480, max_docfreq = 30000)
```


```{r, warning=FALSE}
textplot_wordcloud(dfm_lyricsCorpus_filtered, random_order = FALSE, rotation = 0.25, 
    color = RColorBrewer::brewer.pal(8, "Dark2"))
```


```{r}
# Calculate the document frequency for each term
# This computes how many documents each term appears in
doc_freqs <- docfreq(dfm_lyricsCorpus_filtered)

# Convert to a data frame, sort it, and create a column for term ID or rank
df_doc_freqs <- data.frame(term = names(doc_freqs), doc_freq = doc_freqs) %>%
  arrange(desc(doc_freq)) %>%
  mutate(term_id = row_number())

# Create the plot for document frequencies
ggplot(df_doc_freqs, aes(x = term_id, y = doc_freq)) +
  geom_bar(stat = "identity") +
  labs(title = "Document Frequency Distribution", x = "Term ID", y = "Document Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) # Hide x-axis text for clarity
```

## Optimise for K

```{r}
# Optimise for K
system.time(
  searchK <- searchK(documents = dfm_lyricsCorpus_filtered, 
                     K = c(10,15, 20, 25, 30,40), #specify K to try
                      N = 4800, # Use 10% of observations to determine optimum number of topics
                     proportion = 0.5, # hold out 50% to use for testing
                     heldout.seed = 1234, 
                     M = 10, # number of random initializations to perform
                     cores = 1, # default
                     prevalence =~ s(date) + genre + s(date)*genre,
                     max.em.its = 75, # Max iterations
                     data = meta_data,
                     init.type = "Spectral",
                     verbose=TRUE)
)
```

```{r}
# Extracting the data from the searchK list
K_values <- searchK$results$K  
heldout_likelihood <- searchK$results$heldout
semcoh <- searchK$results$semcoh
residual<- searchK$results$residual
lowerbound <- searchK$results$lbound

# Creating a data frame for plotting
plot_data <- data.frame(
  K = unlist(K_values),
  HeldOutLikelihood = unlist(heldout_likelihood),
  semcoh = unlist(semcoh),
  residual = unlist(residual),
  lowerbound = unlist(lowerbound)
)

# Held-Out Likelihood Plot
plot_heldout <- ggplot(plot_data, aes(x = K, y = HeldOutLikelihood)) +
  geom_line(color = my_palette[1]) +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 90)) +
  labs(x = "",
       y = "Held-Out Likelihood")

# Semantic Coherence Plot
plot_semcoh <- ggplot(plot_data, aes(x = K, y = semcoh)) +
  geom_line(color = my_palette[2]) +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 90)) +
  labs(x = "",
       y = "Semantic Coherence")

# Residual Plot
plot_residual <- ggplot(plot_data, aes(x = K, y = residual)) +
  geom_line(color = my_palette[3]) +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 90)) +
  labs(x = "Number of Topics (K)",
       y = "Residual")

# Lower Bound Plot
plot_lowerbound <- ggplot(plot_data, aes(x = K, y = lowerbound)) +
  geom_line(color = my_palette[4]) +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 90)) +
  labs(x = "Number of Topics (K)",
       y = "Lower Bound")

# Combining the graphs into one plot
combined_plot <- grid.arrange(plot_heldout, plot_semcoh, plot_residual, plot_lowerbound, ncol = 2)

# Combining the graphs into one plot
combined_plot <- grid.arrange(plot_heldout, plot_semcoh, plot_residual, plot_lowerbound, ncol = 2)
ggsave("../figures/Model diagnostics.png", width = 23, height = 15, units = "cm")
```

# Run STM

```{r}
# Assuming dfm_lyricsCorpus_filtered is your document-term matrix
non_empty_docs <- which(rowSums(dfm_lyricsCorpus_filtered) > 0)

# Filter your DFM to keep only non-empty documents
dfm_lyricsCorpus_filtered <- dfm_lyricsCorpus_filtered[non_empty_docs, ]

# Prepare the metadata for STM
meta_data <- data.frame(date = lyrics_data$release.year, genre = lyrics_data$tag)
meta_data <- meta_data[non_empty_docs, ]
```

```{r}
# Run the STM model
stm_model <- stm(documents = dfm_lyricsCorpus_filtered, 
                 data = meta_data, 
                 K = 20, 
                 prevalence = ~ s(date) + genre + s(date)*genre, 
                 seed = 123, init.type = "Spectral")
```
# Analysis

```{r}
# Load data
load(file.path(DIR, '../../data/new model.RData'))
```

## Overall topic proportions

```{r}
topic_labels <- c("Vocal Expressions", "Love", "Feelings and Friendship", "Conversational and Family", 
               "Journey", "Dark Imagery", "Aspiration", "Party", "Male Terms", "Change and Departure",
               "Desire", "Expletives", "Reflection", "Spirituality", "Sociopolitical", "Romance",
               "Street Life and Expletives", "Tonight", "Exclamations", "Knowledge")
```

```{r}
# Plot topic proportions
td_gamma <- tidy(stm_model, matrix = "gamma",
                 document_names = rownames(dfm_lyricsCorpus_filtered))
td_beta <- tidy(stm_model)


top_terms <- td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(7, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()
new_names <- c("Vocal Expressions", "Love", "Feelings and Friendship", "Conversational and Family", 
               "Journey", "Dark Imagery", "Aspiration", "Party", "Male Terms", "Change and Departure",
               "Desire", "Expletives", "Reflection", "Spirituality", "Sociopolitical", "Romance",
               "Street Life and Expletives", "Tonight", "Exclamations", "Knowledge")

top_terms$topic_label <- new_names

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

gamma_terms %>%
  top_n(20, gamma) %>%
  mutate(topic_label = factor(topic_label, levels = rev(unique(topic_label)))) %>%
  ggplot(aes(topic_label, gamma, label = terms, fill = topic_label)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.22)) +
  theme_calc() +
  theme(plot.title = element_text(size = 16),
        plot.subtitle = element_text(size = 13),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = NULL, y = expression("Expected Topic Proportions"))
ggsave("../figures/Topic proportions.png", width = 23, height = 15, units = "cm")
```

```{r}
labelTopics(stm_model, n = 7)
```
1. **Vocal Expressions**: 

2. **Love**: 

3. **Feelings and Friendship**: 

4. **Conversational and Family**: 

5. **Journey**: 

6. **Dark Imagery**: 

7. **Aspiration**: 

8. **Party**:

9. **Male Terms**: 

10. **Change and Departure**: 

11. **Desire**: 

12. **Expletives**: 

13. **Reflection**: 

14. **Spirituality**: 

15. **Sociopolitical**: 

16. **Romance**: 

17. **Street Life and Expletives**: 

18. **Tonight**: 

19. **Exclamations**: 

20. **Knowledge**: 

# Topics over time

```{r}
# Estimate time effect
time_effect <- estimateEffect(1:20 ~ s(date), stm_model, meta = meta_data, uncertainty = "Global")
effect <- extract.estimateEffect(time_effect, "date", model = stm_model, method = "pointestimate")

# Add topic labels
effect <- effect %>%
  mutate(topic_label = topic_labels[topic])

# Add overall topic proportions
gamma_terms_subset <- gamma_terms[, c("topic_label", "gamma")]
effect <- merge(effect, gamma_terms_subset , by = "topic_label", all.x = TRUE)
```

```{r}
topics_data <- effect  
topics_data <- topics_data %>% arrange(desc(gamma))
topics_data$topic_label <- factor(topics_data$topic_label, levels = unique(topics_data$topic_label))

# Now, plot using 'ggplot2' with the sorted 'topic_label'
ggplot(topics_data, aes(x = covariate.value, y = estimate, group = topic, color = as.factor(topic))) +
  geom_line() +
  geom_line(aes(y = ci.lower), linetype = "dashed") +  
  geom_line(aes(y = ci.upper), linetype = "dashed") +
  theme_calc() +
  theme(legend.position = "none") +
  labs(x = "Release Year", y = "Expected Topic Proportions", color = "Topic") +
  facet_wrap(~ topic_label, ncol = 5) 
```









